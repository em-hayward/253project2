---
title: "Statistical Machine Learning Final Project: Gun Violence Data"
author: "Hayley Hadges, Ethan Deutsch, Em Hayward"
date: "5/7/2020"
output: html_document
df_print: paged
---

```{r, echo = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```
```{r,echo=FALSE}
library(tidyverse)
library(janitor)
library(tidyverse) #for plotting and summarizing
library(GGally) #for nice scatterplot matrix 
library(ggridges) #for joy/ridge plots
library(corrplot) #for basic correlation matrix plot
library(naniar) #for exploring missing values
library(pdp) #for partial dependence plots, MARS models

#making things look nice
library(lubridate) #for nice dates
library(knitr) #for nice tables
library(scales) #for nice labels on graphs
library(gridExtra) #for arranging plots
library(broom) #for nice model output

#data
library(ISLR) #for data
library(moderndive) #for data

#modeling
library(rsample) #for splitting data
library(recipes) #for keeping track of any transformations we do
library(caret) #for modeling
library(leaps) #for variable selection
library(glmnet) #for LASSO
library(earth) #for MARS models
library(vip) #NEW for importance plots

library(usmap)
library(ggplot2)
library(tidyverse)

library(rpart)
library(rpart.plot)
library(expss)
library(data.table)
library(formattable)
theme_set(theme_minimal())
```

```{r, echo = FALSE}
gvd <- read_csv("GunViolenceData.csv")
```

In our final project for Statistical Machine Learning, we’re using gun violence data from all US states including the District of Columbia ranging from January of 2018 to March of 2018, which include variables such as number of people killed, state, date, and number of participants just to name a few. With gun violence in the US as a relevant and continuous issue, we chose to analyze this dataset because we believe it is important to look deeper into the issue through data in the attempt to better understand it. To clean the data, we changed the categorical variable 'date' into the numerical variable 'new_date'. We created a new variable 'missing' for all the missing data. From the 'participant_gender' variable, we created the 'female_present' variable. From the 'participant_age' variable we created the 'teen_present' and 'child_present' variables. We also removed multiple variables such as 'address' and 'source_url' that we deemed unusable.    

In our project, we would like to formulate multiple different models that predict the variable n_killed (number of people killed in shooting). Throughout this modeling process, we look at the cross validated root mean squared error (RMSE) of each model and find the lowest to determine the ‘best model’. With this best model we plan to use the testing data to find which variables make the strongest impact on the number of people killed as well as look closer at how these variables affect the n_killed variable. Finding these variables with the largest impact and looking at their effect can be used to better understand gun violence and determine preventative measures for future gun violence. 


## **Exploring Variables**

Before we begin modeling, we first look at some of the variables in the model more closely to determine which could be beneficial to use in the models.

(Include exploratory graphs/illustrations here and describe/analyze)

```{r, echo = FALSE, results = FALSE}
#create a new date column on clean data that is of the Date datatype
datefixed <- gvd
datefixed['new_date']<-as.Date(as.character(gvd$date), format='%m/%d/%Y')
class(datefixed$new_date)
```


Should we delete these since we took out the week and day variables?
```{r}
# gun violence grouped by WEEK arranged in chronological order
# note to reverse order just edit arrange(desc(week))

#totalsByWeek <- datefixed %>%
  #group_by(week) %>%
  #summarize(total=n()) %>%
  #arrange(week)
```

```{r}
#totalsByDay <- datefixed %>%
  #group_by(day) %>%
  #summarize(total=n()) %>%
  #arrange(day)
```



```{r, echo = FALSE}
cleandata <- 
  datefixed%>%
  mutate(female_present = str_detect(participant_gender, "Female")) %>% 
  mutate(teen_present = str_detect(participant_age_group, "Teen 12-17")) %>% 
  mutate(child_present = str_detect(participant_age_group, "Child 0-11")) %>% 
  mutate_if(is.character, replace_na, replace = "missing") %>%
  mutate_if(is.logical, replace_na, replace ="missing") %>% 
  select(-c(incident_id, incident_url_fields_missing, address, incident_url, source_url, incident_characteristics, location_description, notes, participant_name, participant_relationship, sources, congressional_district, state_house_district, state_senate_district))%>%
  drop_na()

```


```{r, echo = FALSE}
set.seed(253) 
gv_split <- initial_split(cleandata, 
                             prop = .5)
gv_train <- training(gv_split)
gv_test <- testing(gv_split)
```


```{r, echo = FALSE, results = FALSE}
colnames(gv_train)
```


```{r, echo = FALSE}
gv_train %>%
  group_by(city_or_county) %>%
  summarize(n=n()) %>%
  arrange(desc(n))
```



```{r,  fig.width = 10, fig.height = 7, echo = FALSE}
plot1 <- gv_train%>%
  mutate(week = week(new_date))%>%
  group_by(week)%>%
  summarise(num_incidences = n())%>%
  ggplot(aes(x= week, y = num_incidences))+geom_col()


plot2 <- gv_train%>%
  mutate(week = week(new_date))%>%
  group_by(week)%>%
  summarise(total_killed = sum(n_killed))%>%
  ggplot(aes(x= week, y = total_killed))+geom_col()


grid.arrange(plot1, plot2)
```

The first exploratory graph here is the number of shooting incidents that happen per week. The second plot, is the number of people killed per week in recorded shooting incidents. In the first graph it seems there is a slightly higher number of incidents that occur in the first five weeks of the year compared to the next eight weeks. There is a similar trend with the number of people killed, as four of the first five weeks have the highest death toll of the enntire 13-week data set. 

```{r, echo = FALSE, results = FALSE}
deaths <- gvd %>% 
  filter(n_killed > 0)

injured <- gvd %>% 
  filter(n_killed < 1) %>%
  filter(n_injured > 0)

nobodyHurt <- gvd %>% 
  filter(n_killed < 1) %>% 
  filter(n_injured < 1)
  
nrow(deaths)
nrow(injured)
nrow(nobodyHurt)
```


```{r, echo = FALSE}
# gun violence grouped by state order descending

dataByState <- datefixed %>%
  group_by(state) %>%
  summarize(n=n()) %>%
  arrange(desc(n))


plot_usmap(data = dataByState, values = "n", color = "black") +
  scale_fill_continuous(low = "white", high = "orchid3", name = "Total Violence (2018)", label = scales::comma) + theme(legend.position = "right")


dataByState <- deaths %>%
  group_by(state) %>%
  summarize(n=n()) %>%
  arrange(desc(n))


plot_usmap(data = dataByState, values = "n", color = "black") +
  scale_fill_continuous(low = "white", high = "orchid3", name = "Number Of Deaths (2018)", label = scales::comma) + theme(legend.position = "right")

dataByState <- injured %>%
  group_by(state) %>%
  summarize(n=n()) %>%
  arrange(desc(n))


plot_usmap(data = dataByState, values = "n", color = "black") +
  scale_fill_continuous(low = "white", high = "orchid3", name = "Number Injured (2018)", label = scales::comma) + theme(legend.position = "right")

dataByState <- nobodyHurt %>%
  group_by(state) %>%
  summarize(n=n()) %>%
  arrange(desc(n))


plot_usmap(data = dataByState, values = "n", color = "black") +
  scale_fill_continuous(low = "white", high = "orchid3", name = "Nobody Hurt (2018)", label = scales::comma) + theme(legend.position = "right")
```

In these four US maps we can start by looking at the total violence, number of deaths by state. The darker states in the first three graphs indicate that there are more deaths and injuries in those states and lighter or white colours means there is less violence overall in those states. The four darkest states on average in the first three graphs are Texas, California, Illinois and Florida. The lighter regions of the United States are in the Mid-Eastern U.S., the upper Northeast and Alaska and Hawai'i. The fourth graph shows the number of people that were not hurt but still in a shooting incident. There is a larger switch of the darkness of colours in this graph which includes a significant darkening of Michigan, Wisconsin and Minnesota and the lightening of many Southern States.

## **Modeling Process**

With these exploratory graphs in mind, we begin to fit models. We first fit a backward stepping model, which uses only numerical variables, to determine the most important variables that remain in the final model. These variables include n_injured, latitude, n_guns_involved, and longitude. Next we fit an ordinary least squares (OLS) linear regression model with these variables. However, since we would like to include categorical variables as well, we fit multiple OLS models with the numerical variables we found in our previous model and differing categorical variables until we find the model with the smallest RMSE. The variables that lead to the smallest RMSE in the OLS model are then used in all future modeling. We create lasso, MARS, tree, and random forest models and compute the cross validated RMSE for each. The following code illustrates how we fit each model and the cross validated RMSE of each.


```{r, results = FALSE}
gv_train_num <- gv_train %>% 
  select_if(is.numeric)

set.seed(253)
splits <- trainControl(method = "cv", number = 5)

#gv_cv_vars <- train(
 # n_killed ~ .,
  #data = gv_train_num,
  #method = "leapBackward",
  #tuneGrid = data.frame(nvmax = 1:8),
  #trControl = splits,
  #na.action = na.omit
#)

#gv_cv_vars$results

#RMSE of 0.5156813
```

```{r, results = FALSE}
#This model uses the four numeric variables from the previous model
set.seed(253) 
gv_ols_basic <- train(
  n_killed ~ n_injured + latitude + n_guns_involved + longitude,
  data = gv_train, 
  method = "lm",
  trControl = splits, 
  na.action = na.omit
)

#RMSE of 0.5064652
```

```{r, results = FALSE}
#This model uses the variables that equate with the smallest RMSE in an OLS model
set.seed(253) 
gv_ols_adv <- train( 
  n_killed ~ n_injured + latitude + n_guns_involved + longitude + female_present + teen_present +  state + new_date,
  data = gv_train, 
  method = "lm",
  trControl = splits, 
  na.action = na.omit
)

#RMSE of 0.4765476		
```

```{r}
summary(gv_ols_adv$finalModel) %>% 
  coefficients() %>% 
  tidy()
```

Using the OLS model, we are able to predict the number of people killed in a given shooting if all of the variables are held at their mean. That means that, on average, 8.375 people are killed in ever shooting incident if each variable is held at its mean. Some noteworthy trends include that for every person injured, on average, .2 less people will die in a shooting incident. Since the estimate is 8.375 per incident, this value changes state to state. The state that would have the lowest estimate per incident is in New Hampshire where the number would drop to 7.935 on average if everything was held the same and the highest estimate is in Nevada where the estiamte would be 8.662 deaths.

```{r, results = FALSE}
lambda_grid <- 10^seq(-4, -1 , length = 50)

set.seed(253)
gv_lasso <- train(
  n_killed ~ n_injured + latitude + n_guns_involved + longitude + female_present + teen_present +  state + new_date,
  data = gv_train, 
  method = "glmnet",
  tuneGrid = data.frame(alpha = 1, lambda = lambda_grid),
  trControl = splits,
  na.action = na.omit,
  metric = "RMSE", 
  maximize = FALSE
)

#RMSE of 0.4762228		
```

```{r}
best_lambda <- gv_lasso$bestTune$lambda
coefficients(gv_lasso$finalModel, s = best_lambda)
```

Given our best tuned Lasso model, we determined that longitude, Teen present and eight states do not make a significant impact on our model. We also find that our lasso model's estimate has a much lower average deaths per incident at 5.75 deaths. The effect of number of people injured in a shooting remains similar to the advanced OLS model. We also learn that if a female is present the number of deaths in a given shooting rises, on average, by .28 deaths. Nevada and New Hampshire both have the largest impact on the original estimate, but New Hampshire has a significantly lower impact than it did in the OLS model while Nevada remains close to the same.

```{r, results = FALSE}
set.seed(253)

gv_mars <- train(
  n_killed ~ n_injured + latitude + n_guns_involved + longitude + female_present + teen_present +  state + new_date,
  data = gv_train, 
  method = "earth",
  trControl = trainControl(method = "cv",
                           number = 5),
  tuneGrid = data.frame(degree = 1, nprune = 4:12),
  na.action = na.omit
)

#RMSE of 0.4653774	
```

```{r}
gv_mars$finalModel %>% 
  coefficients() %>% 
  tidy()
```

```{r, results = FALSE}
set.seed(253)

gv_class_tree <- train(
  n_killed ~   n_injured + latitude + n_guns_involved + longitude + female_present + teen_present +  state + new_date,
  data = gv_train,
  method = "rpart",
  tuneGrid = data.frame(cp = .01),
  trControl = trainControl(method = "cv", number = 5),
  metric = "RMSE",
  na.action = na.omit
)

#RMSE of 0.5219339		
```

```{r}
rpart.plot(gv_class_tree$finalModel, cex = .8, fallen.leaves = FALSE)
```



```{r, results = FALSE}
set.seed(253)
gv_tree_oob <- train(
  n_killed ~ n_injured + latitude + n_guns_involved + longitude + female_present + teen_present +  state + new_date,
  data = gv_train, 
  method = "rf",
  trControl = trainControl(method = "oob"),
  tuneGrid = data.frame(mtry = c(5, 10, 15, 20)),
  ntree = 50, 
  importance = TRUE, 
  nodesize = 5, 
  na.action = na.omit
)

#RMSE of 0.4571935	
```

```{r, echo = FALSE}
gv_tree_oob %>%
  ggplot(aes(x = mtry, y = RMSE)) +
  geom_point()
```


```{r, echo = FALSE, results = FALSE}
plot(gv_tree_oob$finalModel)
```


The following table illustrates the cross validated RMSE for each model type. 

```{r, echo = FALSE}
RMSE <- matrix(c(0.5156813, 0.4765476, 0.4762228, 0.4653774, 0.5219339, 0.4571935))
rownames(RMSE) <- c("Step Backwards","OLS", "Lasso", "Mars", "Tree", "Random Forest")
colnames(RMSE) <- ("RMSE")

RMSE
```


The model we fit with the lowest cross validated RMSE is the random forest model. We will use the test data on this model and look more closely at variable importance and relationship.


## **Best Model Analysis**

Use test data here:
```{r}
vip(gv_tree_oob$finalModel, num_features = 16, bar = FALSE)
```

```{r, echo = FALSE}
p1 <- partial(
  gv_tree_oob, 
  pred.var = "n_injured",
  grid.resolution = 20
  ) %>% 
  autoplot()

p2 <- partial(
  gv_tree_oob, 
  pred.var = "n_guns_involved",
  grid.resolution = 20
  ) %>% 
  autoplot()

p3 <- partial(
  gv_tree_oob, 
  pred.var = "latitude",
  grid.resolution = 20
  ) %>% 
  autoplot()

p4 <- partial(
  gv_tree_oob, 
  pred.var = "new_date",
  grid.resolution = 20
  ) %>% 
  autoplot()

grid.arrange(p1, p2, p3, p4, ncol = 2)
```


To check to see if the model preforms similarly on the testing data, we calculate the cross validated RMSE using the testing data. The RMSE is 0.4926709, which doesn't differ too much from the training RMSE, however it does not look better than the previous models we fit with the training data. 


```{r, echo = FALSE, results = FALSE}
gv_test %>% 
  mutate(pred_n_killed = predict(gv_tree_oob, newdata = gv_test)) %>% 
  summarize(RMSE = sqrt(mean((n_killed - pred_n_killed)^2)))
```


## **Conclusion**

In conclusion we found that the best model to use to predict the number of shooting deaths in the U.S. is the random forest as it had the lowest RMSE. 
