---
title: "Statistical Machine Learning Final Project: Gun Violence Data"
author: "Hayley Hadges, Ethan Deutsch, Em Hayward"
date: "5/7/2020"
output: html_document
df_print: paged
---

```{r}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```
```{r,echo=FALSE}
library(tidyverse)
library(janitor)
library(tidyverse) #for plotting and summarizing
library(GGally) #for nice scatterplot matrix 
library(ggridges) #for joy/ridge plots
library(corrplot) #for basic correlation matrix plot
library(naniar) #for exploring missing values
library(pdp) #for partial dependence plots, MARS models

#making things look nice
library(lubridate) #for nice dates
library(knitr) #for nice tables
library(scales) #for nice labels on graphs
library(gridExtra) #for arranging plots
library(broom) #for nice model output

#data
library(ISLR) #for data
library(moderndive) #for data

#modeling
library(rsample) #for splitting data
library(recipes) #for keeping track of any transformations we do
library(caret) #for modeling
library(leaps) #for variable selection
library(glmnet) #for LASSO
library(earth) #for MARS models
library(vip) #NEW for importance plots

library(usmap)
library(ggplot2)
library(tidyverse)

library(rpart)
library(rpart.plot)
library(expss)
library(data.table)
library(formattable)
theme_set(theme_minimal())
```

```{r, echo = FALSE}
gvd <- read_csv("GunViolenceData.csv")
```
```{r, echo = FALSE, results = FALSE}
levels(gvd$state)
```
```{r, echo = FALSE, results = FALSE}
colnames(gvd)
```

In our final project for Statistical Machine Learning, we’re using gun violence data from all US states including the District of Columbia ranging from January of 2018 to March of 2018, which include variables such as number of people killed, state, date, and number of participants just to name a few. With gun violence in the US as a relevant and continuous issue, we chose to analyze this dataset because we believe it is important to look deeper into the issue through data in the attempt to better understand it. To clean the data, we . . .  

In our project, we would like to formulate multiple different models that predict the variable n_killed (number of people killed in shooting). Throughout this modeling process, we look at the cross validated root mean squared error (RMSE) of each model and find the lowest to determine the ‘best model’. With this best model we plan to use the testing data to find which variables make the strongest impact on the number of people killed as well as look closer at how these variables affect the n_killed variable. Finding these variables with the largest impact and looking at their effect can be used to better understand gun violence and determine preventative measures for future gun violence. 


## **Exploring Variables**

Before we begin modeling, we first look at some of the variables in the model more closely to determine which could be beneficial to use in the models.

(Include exploratory graphs/illustrations here and describe/analyze)

```{r, echo = FALSE, results = FALSE}
#create a new date column on clean data that is of the Date datatype
datefixed <- gvd
datefixed['new_date']<-as.Date(as.character(gvd$date), format='%m/%d/%Y')
class(datefixed$new_date)
```


Should we delete these since we took out the week and day variables?
```{r}
# gun violence grouped by WEEK arranged in chronological order
# note to reverse order just edit arrange(desc(week))

totalsByWeek <- datefixed %>%
  group_by(week) %>%
  summarize(total=n()) %>%
  arrange(week)
```

```{r}
totalsByDay <- datefixed %>%
  group_by(day) %>%
  summarize(total=n()) %>%
  arrange(day)
```




```{r, echo = FALSE}
# gun violence grouped by state order descending

dataByState <- datefixed %>%
  group_by(state) %>%
  summarize(n=n()) %>%
  arrange(desc(n))
```

```{r, echo = FALSE}
cleandata <- 
  datefixed%>%
  mutate(female_present = str_detect(participant_gender, "Female")) %>% 
  mutate(teen_present = str_detect(participant_age_group, "Teen 12-17")) %>% 
  mutate(child_present = str_detect(participant_age_group, "Child 0-11")) %>% 
  mutate_if(is.character, replace_na, replace = "missing") %>%
  mutate_if(is.logical, replace_na, replace ="missing") %>% 
  select(-c(incident_id, incident_url_fields_missing, address, incident_url, source_url, incident_characteristics, location_description, notes, participant_name, participant_relationship, sources))

```

```{r, echo = FALSE}
set.seed(253) 
gv_split <- initial_split(cleandata, 
                             prop = .5)
gv_train <- training(gv_split)
gv_test <- testing(gv_split)
```



```{r, echo = FALSE, results = FALSE}
colnames(gv_train)

# gives the data type of each column, helpful for debugging weird errors with types. Many of the types are factors
# but many functions cant operate on factors.
map(gv_train, class)
```




```{r}
library(tidyverse)
#cleandata$participant_gender <- str_replace(cleandata$participant_gender, "||", ",")
#cleandata$participant_gender <- str_replace(cleandata$participant_gender, "::", "")
#cleandata$participant_gender <- str_replace(cleandata$participant_gender, "[0123]", "")

```



```{r, echo = FALSE}
gv_train %>%
  group_by(city_or_county) %>%
  summarize(n=n()) %>%
  arrange(desc(n))
```


Should we make plots with categorical variables?
```{r, fig.width = 14, fig.height = 10}
plot1 <- gv_train %>%
  ggplot(aes(x=date, y=n_killed)) +
  geom_point()

plot2 <- gv_train %>%
  ggplot(aes(x=date, y=n_injured)) +
  geom_point()


plot4 <- gv_train %>%
  ggplot(aes(x = (latitude/longitude), y = n_killed)) +
  geom_point()

grid.arrange(plot1, plot2, plot4, ncol = 2)
```


```{r}
plot_usmap(data = dataByState, values = "n", color = "black") +
  scale_fill_continuous(low = "white", high = "orchid3", name = "Total Violence (2018)", label = scales::comma) + theme(legend.position = "right")
```
```{r, echo = FALSE}
deaths <- gvd %>% 
  filter(n_killed > 0)

injured <- gvd %>% 
  filter(n_killed < 1) %>%
  filter(n_injured > 0)

nobodyHurt <- gvd %>% 
  filter(n_killed < 1) %>% 
  filter(n_injured < 1)
  
nrow(deaths)
nrow(injured)
nrow(nobodyHurt)
```
```{r, echo = FALSE}
dataByState <- deaths %>%
  group_by(state) %>%
  summarize(n=n()) %>%
  arrange(desc(n))


plot_usmap(data = dataByState, values = "n", color = "black") +
  scale_fill_continuous(low = "white", high = "orchid3", name = "Number Of Deaths (2018)", label = scales::comma) + theme(legend.position = "right")

dataByState <- injured %>%
  group_by(state) %>%
  summarize(n=n()) %>%
  arrange(desc(n))


plot_usmap(data = dataByState, values = "n", color = "black") +
  scale_fill_continuous(low = "white", high = "orchid3", name = "Number Injured (2018)", label = scales::comma) + theme(legend.position = "right")

dataByState <- nobodyHurt %>%
  group_by(state) %>%
  summarize(n=n()) %>%
  arrange(desc(n))


plot_usmap(data = dataByState, values = "n", color = "black") +
  scale_fill_continuous(low = "white", high = "orchid3", name = "Nobody Hurt (2018)", label = scales::comma) + theme(legend.position = "right")
```
```{r, echo = FALSE}
dataByCounty <- datefixed %>%
  group_by(city_or_county) %>%
  summarize(n=n()) %>%
  arrange(desc(n))
```


## **Modeling Process**

With these exploratory graphs in mind, we begin to create models. We first fit a backward stepping model, which uses only numerical variables, to determine the most important variables that remain in the final model. These variables include n_injured, latitude, n_guns_involved, and longitude. Next we fit an ordinary least squares (OLS) linear regression model with these variables. However, since we would like to include categorical variables as well, we fit multiple OLS models with the numerical variables we found in our previous model and differing categorical variables until we find the model with the smallest RMSE. The variables that lead to the smallest RMSE in the OLS model are then used in all future modeling. We create lasso, MARS, tree, and random forest models and compute the cross validated RMSE for each. The following code illustrates how we fit each model.


```{r, results = FALSE}
gv_train_num <- gv_train %>% 
  select_if(is.numeric)

set.seed(253)
splits <- trainControl(method = "cv", number = 5)

gv_cv_vars <- train(
  n_killed ~ .,
  data = gv_train_num,
  method = "leapBackward", #NEW method!
  tuneGrid = data.frame(nvmax = 1:8),
  trControl = splits,
  na.action = na.omit
)

summary(gv_cv_vars)
#RMSE of 0.5156813
```

```{r, results = FALSE}
#This model uses the four numeric variables from the previous model
set.seed(253) 
gv_ols <- train(
  n_killed ~ n_injured + latitude + n_guns_involved + longitude,
  data = gv_train, 
  method = "lm",
  trControl = splits, 
  na.action = na.omit
)

#RMSE of 0.5039284	
```

```{r, results = FALSE}
#This model uses the variables that equate with the smallest RMSE in an OLS model
set.seed(253) 
gv_ols2 <- train( 
  n_killed ~ n_injured + latitude + n_guns_involved + longitude + female_present + teen_present +  state + new_date,
  data = gv_train, 
  method = "lm",
  trControl = splits, 
  na.action = na.omit
)

#RMSE of 0.4736671	
```

```{r, results = FALSE}
lambda_grid <- 10^seq(-4, -1 , length = 50)

set.seed(253)
gv_lasso <- train(
  n_killed ~ n_injured + latitude + n_guns_involved + longitude + female_present + teen_present +  state + new_date,
  data = gv_train, 
  method = "glmnet",
  tuneGrid = data.frame(alpha = 1, lambda = lambda_grid),
  trControl = splits,
  na.action = na.omit,
  metric = "RMSE", 
  maximize = FALSE
)

#RMSE of 0.4732095		
```

```{r, results = FALSE}
set.seed(253)

gv_mars <- train(
  n_killed ~ n_injured + latitude + n_guns_involved + longitude + female_present + teen_present +  state + new_date,
  data = gv_train, 
  method = "earth",
  trControl = trainControl(method = "cv",
                           number = 5),
  tuneGrid = data.frame(degree = 1, nprune = 4:12),
  na.action = na.omit
)

#RMSE of 0.4640177
```


```{r, results = FALSE}
set.seed(253)

gv_class_tree <- train(
  n_killed ~   n_injured + latitude + n_guns_involved + longitude + female_present + teen_present +  state + new_date,
  data = gv_train,
  method = "rpart",
  tuneGrid = data.frame(cp = .5),
  trControl = trainControl(method = "cv", number = 5),
  metric = "RMSE",
  na.action = na.omit
)

#RMSE of 0.519345	
```


```{r, results = FALSE}
set.seed(253)
gv_tree_oob <- train(
  n_killed ~ n_injured + latitude + n_guns_involved + longitude + female_present + teen_present +  state + new_date,
  data = gv_train, 
  method = "rf",
  trControl = trainControl(method = "oob"),
  tuneGrid = data.frame(mtry = 10),
  ntree = 50, 
  importance = TRUE, 
  nodesize = 5, 
  na.action = na.omit
)

#RMSE of 0.4452655	
```

```{r, echo = FALSE, results = FALSE}
plot(gv_tree_oob$finalModel)
```


The following table illustrates the cross validated RMSE for each model type. 

```{r, echo = FALSE}
RMSE <- matrix(c(0.5156813, 0.4736671, 0.4732095, 0.4640177, 0.519345, 0.445265	))
rownames(RMSE) <- c("Step Backwards","OLS", "Lasso", "Mars", "Tree", "Random Forest")
colnames(RMSE) <- ("RMSE")

RMSE
```


The model we fit with the lowest cross validated RMSE is the random forest model. We will use the test data on this model and look more closely at variable importance and relationship.


## **Best Model Analysis**

Use test data here:
```{r}
vip(gv_tree_oob$finalModel, num_features = 16, bar = FALSE)
```

```{r, echo = FALSE}
p1 <- partial(
  gv_tree_oob, 
  pred.var = "n_injured",
  grid.resolution = 20
  ) %>% 
  autoplot()

p2 <- partial(
  gv_tree_oob, 
  pred.var = "n_guns_involved",
  grid.resolution = 20
  ) %>% 
  autoplot()

p3 <- partial(
  gv_tree_oob, 
  pred.var = "latitude",
  grid.resolution = 20
  ) %>% 
  autoplot()

p4 <- partial(
  gv_tree_oob, 
  pred.var = "new_date",
  grid.resolution = 20
  ) %>% 
  autoplot()

grid.arrange(p1, p2, p3, p4, ncol = 2)
```


## **Results**